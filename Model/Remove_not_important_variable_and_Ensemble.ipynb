{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove not important variable And Ensemble\n",
    "- catboost 결과 중요하지 않은 상위 99개 변수를 제거한 모델\n",
    "- 필요없는 열을 제거하자, score가 소폭 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family = 'Malgun Gothic')\n",
    "\n",
    "# catboostregressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# fix random seed\n",
    "import random\n",
    "random.seed(2020)\n",
    "random_seed = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18684\\2331995673.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_y.rename(columns = {\"datetime\":\"forecast\"}, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"C:/dust/data/train.csv\")\n",
    "train = train.fillna(method = 'ffill')\n",
    "train.drop(columns = [\"Unnamed: 0\",\"PM25\"], inplace = True)\n",
    "train = train.sort_values(by = [\"datetime\"])\n",
    "\n",
    "train_y = train[[\"datetime\",\"PM10\"]]\n",
    "train_y.rename(columns = {\"datetime\":\"forecast\"}, inplace = True)\n",
    "train_x = train.rename(columns = {\"PM10\":\"actual_PM10_before1\"})\n",
    "train_x = train_x.shift(1)\n",
    "train = pd.concat([train_x, train_y],axis = 1)\n",
    "train = train.dropna()\n",
    "train = train.reset_index(drop = True)\n",
    "train[\"datetime\"] = pd.to_datetime(train[\"datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18684\\2652636580.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_y.rename(columns = {\"datetime\":\"forecast\"}, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"C:/dust/data/test.csv\")\n",
    "test = test.fillna(method = 'ffill')\n",
    "test.drop(columns = [\"Unnamed: 0\",\"PM25\"], inplace = True)\n",
    "test = test.sort_values(by = [\"datetime\"])\n",
    "\n",
    "test_y = test[[\"datetime\",\"PM10\"]]\n",
    "test_y.rename(columns = {\"datetime\":\"forecast\"}, inplace = True)\n",
    "test_x = test.rename(columns = {\"PM10\":\"actual_PM10_before1\"})\n",
    "test_x = test_x.shift(1)\n",
    "test = pd.concat([test_x, test_y],axis = 1)\n",
    "test = test.dropna()\n",
    "test = test.reset_index(drop = True)\n",
    "test[\"datetime\"] = pd.to_datetime(test[\"datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shifting \n",
    "- 1단위 차분시 3차시 correlation을 features로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data shifting\n",
    "train_shift2 = train[[\"datetime\",\"actual_PM10_before1\"]].shift(1)\n",
    "train_shift3 = train[[\"datetime\",\"actual_PM10_before1\"]].shift(2)\n",
    "train_shift4 = train[[\"datetime\",\"actual_PM10_before1\"]].shift(3)\n",
    "# test data shifting\n",
    "test_shift2 = test[[\"datetime\",\"actual_PM10_before1\"]].shift(1)\n",
    "test_shift3 = test[[\"datetime\",\"actual_PM10_before1\"]].shift(2)\n",
    "test_shift4 = test[[\"datetime\",\"actual_PM10_before1\"]].shift(3)\n",
    "# features renaming\n",
    "train_shift2.rename(columns = {\"actual_PM10_before1\":\"actual_PM10_before2\"}, inplace = True)\n",
    "train_shift3.rename(columns = {\"actual_PM10_before1\":\"actual_PM10_before3\"}, inplace = True)\n",
    "test_shift2.rename(columns = {\"actual_PM10_before1\":\"actual_PM10_before2\"}, inplace = True)\n",
    "test_shift3.rename(columns = {\"actual_PM10_before1\":\"actual_PM10_before3\"}, inplace = True)\n",
    "# drop columns :: datetime\n",
    "train_shift2.drop(columns = [\"datetime\"], inplace = True)\n",
    "train_shift3.drop(columns = [\"datetime\"], inplace = True)\n",
    "test_shift2.drop(columns = [\"datetime\"], inplace = True)\n",
    "test_shift3.drop(columns = [\"datetime\"], inplace = True)\n",
    "train_shift2.reset_index(drop = True, inplace = True)\n",
    "train_shift3.reset_index(drop = True, inplace = True)\n",
    "test_shift2.reset_index(drop = True, inplace = True)\n",
    "test_shift3.reset_index(drop = True, inplace = True)\n",
    "# concat for merging to Modeling DataSet\n",
    "train = pd.concat([train,train_shift2,train_shift3],axis = 1)\n",
    "test = pd.concat([test,test_shift2,test_shift3],axis = 1)\n",
    "# drop time at not data\n",
    "train = train.dropna()\n",
    "test = test.dropna()\n",
    "# Merging to Modeling DataSet\n",
    "concat_actual_train = train.drop(columns = [\"forecast\",\"PM10\"])\n",
    "concat_actual_test = test.drop(columns = [\"forecast\",\"PM10\"])\n",
    "# rename columns for not duplicates columns\n",
    "datetime_index_train = concat_actual_train[\"datetime\"]\n",
    "concat_actual_train = concat_actual_train.drop(columns = [\"datetime\"])\n",
    "concat_actual_train.columns = [i + \"_1\" for i in concat_actual_train.columns]\n",
    "concat_actual_train = pd.concat([datetime_index_train, concat_actual_train], axis = 1)\n",
    "datetime_index_test = concat_actual_test[\"datetime\"]\n",
    "concat_actual_test = concat_actual_test.drop(columns = [\"datetime\"])\n",
    "concat_actual_test.columns = [i + \"_1\" for i in concat_actual_test.columns]\n",
    "concat_actual_test = pd.concat([datetime_index_test, concat_actual_test], axis = 1)\n",
    "# concat time and target\n",
    "train_for_concat = train[[\"datetime\",\"PM10\"]]\n",
    "test_for_concat = test[[\"datetime\",\"PM10\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resid, Trend, Seasonal\n",
    "- 앞전에 시계열 분해에서 얻었던 columns 별 Residual, Trend, Seasonal을 Modeling Data와 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Residual, Trend, Seasonal\n",
    "resid_train = pd.read_csv(\"C:/dust/model/resid_train.csv\")\n",
    "resid_test = pd.read_csv(\"C:/dust/model/resid_test.csv\")\n",
    "trend_train = pd.read_csv(\"C:/dust/model/trend_train.csv\")\n",
    "trend_test = pd.read_csv(\"C:/dust/model/trend_test.csv\")\n",
    "seasonal_train = pd.read_csv(\"C:/dust/model/seasonal_train.csv\")\n",
    "seasonal_test = pd.read_csv(\"C:/dust/model/seasonal_test.csv\")\n",
    "# Change to Datetime type\n",
    "resid_train[\"datetime\"] = pd.to_datetime(resid_train[\"datetime\"])\n",
    "resid_test[\"datetime\"] = pd.to_datetime(resid_test[\"datetime\"])\n",
    "trend_train[\"datetime\"] = pd.to_datetime(trend_train[\"datetime\"])\n",
    "trend_test[\"datetime\"] = pd.to_datetime(trend_test[\"datetime\"])\n",
    "seasonal_train[\"datetime\"] = pd.to_datetime(seasonal_train[\"datetime\"])\n",
    "seasonal_test[\"datetime\"] = pd.to_datetime(seasonal_test[\"datetime\"])\n",
    "# merging Residual, Trend, Seasonal\n",
    "train = pd.merge(resid_train,train_for_concat, on = \"datetime\", how = \"inner\")\n",
    "test = pd.merge(resid_test,test_for_concat, on = \"datetime\", how = \"inner\")\n",
    "train = pd.merge(trend_train,train, on = \"datetime\", how = \"inner\")\n",
    "test = pd.merge(trend_test,test, on = \"datetime\", how = \"inner\")\n",
    "train = pd.merge(seasonal_train,train, on = \"datetime\", how = \"inner\")\n",
    "test = pd.merge(seasonal_test,test, on = \"datetime\", how = \"inner\")\n",
    "# merge to Modeling DataSet\n",
    "train = pd.merge(train, concat_actual_train, on = \"datetime\", how = \"inner\")\n",
    "test = pd.merge(test, concat_actual_test, on = \"datetime\", how = \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime Features\n",
    "- Datetime type에서 각 시간에 해당하는 월,일,시간을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Set\n",
    "train[\"month\"] = train[\"datetime\"].dt.month\n",
    "train[\"day\"] = train[\"datetime\"].dt.day\n",
    "train[\"hour\"] = train[\"datetime\"].dt.hour\n",
    "# Test Set\n",
    "test[\"month\"] = test[\"datetime\"].dt.month\n",
    "test[\"day\"] = test[\"datetime\"].dt.day\n",
    "test[\"hour\"] = test[\"datetime\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Transformation to Target\n",
    "- Target값을 로그 변환을 취해줘서 outlier와 변동에 모델이 견고하도록 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_x = train.drop(columns = [\"datetime\",\"PM10\"])\n",
    "train_y = train[[\"PM10\"]]\n",
    "train_y = np.log1p(train_y)\n",
    "# Test\n",
    "test_x = test.drop(columns = [\"datetime\",\"PM10\"])\n",
    "test_y = test[[\"PM10\"]]\n",
    "test_y = np.log1p(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Pool To Learning Catboost\n",
    "- Catboost 모델이 범주형 자료를 인식하도록 하기 위해선 features들을 CatPool시켜줘야함\n",
    "- 범주형 변수에 해당하는 16방위와 10분위를 CatPool시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "cart_train = []\n",
    "for i in train_x.columns :\n",
    "    if \"16방위\" in i :\n",
    "        cart_train.append(i)\n",
    "    elif \"10분위\" in i :\n",
    "        cart_train.append(i)\n",
    "\n",
    "# Test\n",
    "cart_test = []\n",
    "for i in test_x.columns :\n",
    "    if \"16방위\" in i :\n",
    "        cart_test.append(i)\n",
    "    elif \"10분위\" in i :\n",
    "        cart_test.append(i)\n",
    "\n",
    "# Switch numbers to string\n",
    "for i in cart_train :\n",
    "    train_x[i] = train_x[i].astype(\"str\")\n",
    "    test_x[i] = test_x[i].astype(\"str\")\n",
    "\n",
    "# CatPool\n",
    "pool_train = Pool(train_x, train_y, cat_features = cart_train)\n",
    "pool_test = Pool(test_x, cat_features = cart_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unimportant variables\n",
    "- 중요하지 않은 변수들은 과감하게 분석에서 제외한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.read_csv(\"C:/dust/model/feature.csv\")\n",
    "col = list(feature.iloc[:99]['columns'])\n",
    "train_x = train_x[col]\n",
    "test_x = test_x[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "- iterations = 2000으로 catboost 시켰으며, 나머지 파라미터의 경우 모두 기본 파라미터를 사용\n",
    "- GPU로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "model = CatBoostRegressor(iterations = 2000, learning_rate = 0.1, depth = 10,random_state = 42 ,loss_function = 'RMSE',task_type='GPU').fit(train_x, train_y)\n",
    "# prediction\n",
    "predict_y = model.predict(test_x)\n",
    "# save\n",
    "model.save_model(\"C:/dust/model/catboost_model.cbm\")\n",
    "# Reverse Transformation for check real results\n",
    "test_y = np.expm1(test_y)\n",
    "predict_y = np.expm1(predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### variance important\n",
    "- 변수 중요도를 시각화하고, 어떠한 변수가 가장 영향을 많이 미치는지 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "feature_importance = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.barh(train_x.columns[sorted_idx], feature_importance[sorted_idx])\n",
    "plt.xlabel(\"CatBoost Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to DataFrame\n",
    "feature_df = pd.DataFrame()\n",
    "feature_df[\"columns\"] = list(train_x.columns[sorted_idx])\n",
    "feature_df[\"score\"] = list(feature_importance[sorted_idx])\n",
    "feature_df.sort_values(by = \"score\", ascending = False, inplace = True)\n",
    "feature_df.reset_index(drop = True, inplace = True)\n",
    "feature_df.to_csv(\"feature.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RMSE and r2 score\n",
    "print(mean_squared_error(test_y, predict_y, squared = False))\n",
    "print(r2_score(test_y, predict_y))\n",
    "# Saving the result\n",
    "pd.DataFrame(predict_y).to_csv(\"C:/dust/model/predict_y_not_remove.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization results\n",
    "plt.plot(test_y, label = \"실제값\")\n",
    "plt.plot(predict_y, label = \"예측값\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "- 모든 변수를 사용한 모형과 중요하지 않은 변수를 삭제조치한 모형 2개를 앙상블\n",
    "- 성능이 소폭 개선되었고, Outlier에 견고해지는 것을 확인할 수 있었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Baseline results\n",
    "predict_y_not_remove = pd.read_csv(\"C:/dust/model/predict_y_not_remove.csv\")\n",
    "# Ensemble\n",
    "predict_esb = (np.array(predict_y_not_remove[[\"0\"]]).reshape(-1) + predict_y) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print RMSE and r2 score\n",
    "print(mean_squared_error(test_y, predict_esb, squared = False))\n",
    "print(r2_score(test_y, predict_esb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization results\n",
    "plt.plot(test_y, label = \"실제값\")\n",
    "plt.plot(predict_esb, label = \"예측값\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8c8540e0960871b600e3f40e1e37dd4369b4892e9ee484b4784a47d7408b04b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
